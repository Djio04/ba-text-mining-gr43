{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "```\n",
    "**answer:** VADER took every word seperately and evaluated the sentiment, it likely took the word 'love' and evaluated it as very positive, and the rest of the words as slightly positive/ generally neutral (neg score 0.0 implies it did not evaluate any word as negative), giving the whole sentence a compound score of around 0.64\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "```\n",
    "**answer:** VADER likely recognized 'don't' and 'love' together to evaluate it as quite negative, it's interesting to note though that it does recognize that sentence 2 is negative but less strongly than sentence 1 being positive, as sentence 1 is definetly strongly positive but sentence 2 is generally negative but 'don't love' does not imply a strong dislike.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "```\n",
    "**answer:** VADER likely evaluated the sentence so much more positive in comparison to sentence 1 due to the addition of the emoticon, since it is also able to handle and evaluate them\n",
    "```\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "```\n",
    "**answer:** I think VADER is quite poor at handling words with multiple meanings, like the word \"ruins\". Though the sentence is quite accuratly evaluated as neutral, it still leans quite close towards negative, since a house being a ruin could be a negative thing, it's well evaluated in my opinion.\n",
    "```\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "```\n",
    "**answer:** This is one of those sentences where the evaluation doesn't seem to make much sense. VADER evaluated words it considers very positive like 'certainly' well and took the rest of the sentence as neutral, but it likely tried negating 'not (negative) considered ruins(negative)' resulting in the sentence having a positive compound evaluation\n",
    "```\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "```\n",
    "**answer:** Although this definitely seems like a neutral sentence, it's really likely the word \"lies\" is considered negative in VADER's lexicon, probably as the human evaluators thought of the word lies as in lying, not lying down, resulting in a negative compound score.\n",
    "```\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```\n",
    "**answer:** Although the neutral compound score is definetly accurate, it's possible the slight positive bias comes from the word 'like'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': 'oh that resist choreo is READYYYYY', 'tweet_url': 'https://x.com/tyuntete/status/1895414524733481051'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = ''# run vader\n",
    "    vader_label = ''# convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "# use scikit-learn's classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tammy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tammy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tammy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\tammy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tammy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tammy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\Tammy\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import pathlib\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "import os\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "airline_tweets_folder = \"C:/Users/Tammy/OneDrive - John Abbott College/YEAR 1/Documents/GitHub/ba-text-mining-gr43/lab_sessions/lab3/airlinetweets/airlinetweets\"\n",
    "categories = [\"negative\", \"neutral\", \"positive\"]\n",
    "tweets_data = []\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    folder_path = f\"{airline_tweets_folder}/{category}\"\n",
    "    if os.path.exists(folder_path):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, encoding=\"utf-8\") as file:\n",
    "                    tweet_text = file.read().strip()\n",
    "                    tweets_data.append((tweet_text, category))\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_score(tweet):\n",
    "    return sia.polarity_scores(tweet)\n",
    "\n",
    "def get_label(score):\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "def print_classification_report(scores, label):\n",
    "    predictions = [] \n",
    "    true_labels = [] \n",
    "    \n",
    "    for tweet, category, score in scores:\n",
    "        if score['compound'] >= 0.05:\n",
    "            predictions.append('positive')  \n",
    "        elif score['compound'] <= -0.05:\n",
    "            predictions.append('negative')\n",
    "        else:\n",
    "            predictions.append('neutral')  \n",
    "        \n",
    "        \n",
    "        true_labels.append(category)\n",
    "    \n",
    "    \n",
    "    if not true_labels or not predictions:\n",
    "        print(f\"Error: true_labels or predictions are empty for {label} tweets.\")\n",
    "        print(f\"true_labels: {true_labels}\")\n",
    "        print(f\"predictions: {predictions}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Classification Report for {label} Tweets:\")\n",
    "    print(classification_report(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Raw Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.49      0.61      1750\n",
      "     neutral       0.58      0.53      0.55      1515\n",
      "    positive       0.57      0.87      0.69      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.66      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on airline tweets\n",
    "raw_scores = []\n",
    "\n",
    "for tweet, category in tweets_data:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    \n",
    "    raw_scores.append((tweet, category, score))\n",
    "print_classification_report(raw_scores, \"Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Lemmatized Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.49      0.60      1750\n",
      "     neutral       0.58      0.52      0.55      1515\n",
      "    positive       0.56      0.87      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.64      0.63      0.61      4755\n",
      "weighted avg       0.65      0.62      0.61      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running VADER on lemmatized tweets\n",
    "def lemmatize_text(tweet):\n",
    "    words = tweet.split()\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "lemmatized_tweets_data = []\n",
    "\n",
    "for tweet, category in tweets_data:\n",
    "    lemmatized_tweet = lemmatize_text(tweet)\n",
    "    lemmatized_tweets_data.append((lemmatized_tweet, category))\n",
    "\n",
    "lemmatized_scores = []\n",
    "\n",
    "for tweet, category in lemmatized_tweets_data:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    lemmatized_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(lemmatized_scores, \"Lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Adjective Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.15      0.26      1750\n",
      "     neutral       0.49      0.78      0.60      1515\n",
      "    positive       0.35      0.47      0.40      1490\n",
      "\n",
      "    accuracy                           0.45      4755\n",
      "   macro avg       0.55      0.47      0.42      4755\n",
      "weighted avg       0.57      0.45      0.41      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on only adjectives\n",
    "def extract_pos(tweet, pos_tags):\n",
    "    words = word_tokenize(tweet)\n",
    "    tagged = pos_tag(words)   \n",
    "    filtered_words = []\n",
    "    for word, tag in tagged:\n",
    "        if tag in pos_tags:\n",
    "            filtered_words.append(word)    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "adjective_tweets = []\n",
    "\n",
    "for tweet, category in tweets_data:\n",
    "    adjectives_only_tweet = extract_pos(tweet, {'JJ', 'JJR', 'JJS'})\n",
    "    adjective_tweets.append((adjectives_only_tweet, category))\n",
    "\n",
    "adjective_scores = []\n",
    "\n",
    "for tweet, category in adjective_tweets:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    adjective_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(adjective_scores, \"Adjective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Lemmatized Adjective Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.15      0.26      1750\n",
      "     neutral       0.49      0.78      0.60      1515\n",
      "    positive       0.35      0.47      0.40      1490\n",
      "\n",
      "    accuracy                           0.45      4755\n",
      "   macro avg       0.55      0.47      0.42      4755\n",
      "weighted avg       0.57      0.45      0.41      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on only adjectives, lemmatized \n",
    "lemmatized_adjective_tweets = []\n",
    "\n",
    "for tweet, category in adjective_tweets:\n",
    "    lemmatized_tweet = lemmatize_text(tweet)\n",
    "    lemmatized_adjective_tweets.append((lemmatized_tweet, category))\n",
    "\n",
    "lemmatized_adjective_scores = []\n",
    "\n",
    "for tweet, category in lemmatized_adjective_tweets:\n",
    "    score = get_sentiment_score(tweet) \n",
    "    lemmatized_adjective_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(lemmatized_adjective_scores, \"Lemmatized Adjective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Noun Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.22      0.35      1750\n",
      "     neutral       0.42      0.71      0.52      1515\n",
      "    positive       0.55      0.62      0.58      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.58      0.52      0.48      4755\n",
      "weighted avg       0.59      0.50      0.48      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on only nouns \n",
    "noun_tweets = []\n",
    "\n",
    "for tweet, category in tweets_data:\n",
    "    noun_tweet = extract_pos(tweet, {'NN', 'NNS', 'NNP', 'NNPS'})\n",
    "    noun_tweets.append((noun_tweet, category))\n",
    "\n",
    "noun_scores = []\n",
    "\n",
    "for tweet, category in noun_tweets:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    noun_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(noun_scores, \"Noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Lemmatized Noun Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.23      0.35      1750\n",
      "     neutral       0.42      0.70      0.52      1515\n",
      "    positive       0.55      0.62      0.58      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.58      0.52      0.49      4755\n",
      "weighted avg       0.59      0.50      0.48      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on only nouns, lemmatized \n",
    "lemmatized_noun_tweets = []\n",
    "for tweet, category in noun_tweets:\n",
    "    lemmatized_tweet = lemmatize_text(tweet)\n",
    "    lemmatized_noun_tweets.append((lemmatized_tweet, category))\n",
    "\n",
    "lemmatized_noun_scores = []\n",
    "\n",
    "for tweet, category in lemmatized_noun_tweets:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    lemmatized_noun_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(lemmatized_noun_scores, \"Lemmatized Noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Verb Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.19      0.30      1750\n",
      "     neutral       0.38      0.78      0.51      1515\n",
      "    positive       0.30      0.24      0.27      1490\n",
      "\n",
      "    accuracy                           0.39      4755\n",
      "   macro avg       0.47      0.40      0.36      4755\n",
      "weighted avg       0.49      0.39      0.36      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on only verbs\n",
    "\n",
    "verb_tweets = []\n",
    "for tweet, category in tweets_data:\n",
    "    verb_tweet = extract_pos(tweet, {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'})\n",
    "    verb_tweets.append((verb_tweet, category))\n",
    "\n",
    "verb_scores = []\n",
    "\n",
    "for tweet, category in verb_tweets:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    verb_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(verb_scores, \"Verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Lemmatized Verb Tweets:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.18      0.29      1750\n",
      "     neutral       0.38      0.77      0.51      1515\n",
      "    positive       0.30      0.25      0.27      1490\n",
      "\n",
      "    accuracy                           0.39      4755\n",
      "   macro avg       0.47      0.40      0.36      4755\n",
      "weighted avg       0.48      0.39      0.35      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VADER on only verbs, lemmatized \n",
    "lemmatized_verb_tweets = []\n",
    "\n",
    "for tweet, category in verb_tweets:\n",
    "    lemmatized_tweet = lemmatize_text(tweet)\n",
    "    lemmatized_verb_tweets.append((lemmatized_tweet, category))\n",
    "\n",
    "lemmatized_verb_scores = []\n",
    "\n",
    "for tweet, category in lemmatized_verb_tweets:\n",
    "    score = get_sentiment_score(tweet)\n",
    "    lemmatized_verb_scores.append((tweet, category, score))\n",
    "\n",
    "print_classification_report(lemmatized_verb_scores, \"Lemmatized Verb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the scores - answers\n",
    "\n",
    "1. Looking at the scores and comparing them overall, we can see that adjectives have higher scores for all categories compared to verbs and nouns, which tells us that sentiment is expressed the best with adjectives. If comparing nouns and verbs, nouns have slightly higher macro and weighted averages for all categories (precision, recall and f1 score), only the weighted average for recall is the exact same (0.54).\n",
    "2. In theory, lemmatisation does help - it simplifies the words by transforming it to its root form, thus it reduces variation and vocabulary becomes less complicated. The only downside could be that some words might lose their actual precise meaning if the prefix or sufix is removed. However, in this case above, the scores for raw and lemmatized tweets are the same; lemmatization does not impact the scores at all.\n",
    "3. Not all parts of speech equally important for sentiment analysis - the most important are adjectives, then nouns and verbs which add context as well, but for instance pronouns or prepositions do not contribute to sentiment analysis (words like \"in\", \"at\", \"she\" do not express emotion at all, while \"wonderful\" and \"joy\" do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for TF-IDF (min_df=2):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.85       350\n",
      "     neutral       0.79      0.73      0.76       303\n",
      "    positive       0.88      0.80      0.84       298\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.81      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report for TF-IDF (min_df=5):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85       350\n",
      "     neutral       0.79      0.73      0.76       303\n",
      "    positive       0.89      0.79      0.83       298\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.81      0.81       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report for TF-IDF (min_df=10):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.92      0.86       350\n",
      "     neutral       0.78      0.75      0.76       303\n",
      "    positive       0.87      0.78      0.82       298\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.81      0.81       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report for Bag of Words (min_df=2):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.90      0.86       350\n",
      "     neutral       0.79      0.75      0.77       303\n",
      "    positive       0.84      0.79      0.81       298\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.81      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report for Bag of Words (min_df=5):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.87      0.84       350\n",
      "     neutral       0.75      0.76      0.76       303\n",
      "    positive       0.85      0.77      0.81       298\n",
      "\n",
      "    accuracy                           0.80       951\n",
      "   macro avg       0.81      0.80      0.80       951\n",
      "weighted avg       0.81      0.80      0.80       951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report for Bag of Words (min_df=10):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.89      0.86       350\n",
      "     neutral       0.77      0.76      0.76       303\n",
      "    positive       0.84      0.77      0.80       298\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.81      0.81      0.81       951\n",
      "weighted avg       0.81      0.81      0.81       951\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_path = \"C:/Users/lisam/OneDrive/Year 3/Period 4/Text Mining/ba-text-mining-gr43/lab_sessions/lab3/airlinetweets\"\n",
    "\n",
    "def load_data(data_path):\n",
    "    texts, labels = [], []\n",
    "    for sentiment in ['negative', 'neutral', 'positive']:\n",
    "        folder_path = os.path.join(data_path, sentiment)\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    texts.append(f.read().strip())\n",
    "                    labels.append(sentiment)\n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = load_data(data_path)\n",
    "\n",
    "map_labels = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "y = np.array([map_labels[label] for label in labels])\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "vectorizers = {\n",
    "    \"TF-IDF (min_df=2)\": TfidfVectorizer(min_df=2),\n",
    "    \"TF-IDF (min_df=5)\": TfidfVectorizer(min_df=5),\n",
    "    \"TF-IDF (min_df=10)\": TfidfVectorizer(min_df=10),\n",
    "    \"Bag of Words (min_df=2)\": CountVectorizer(min_df=2),\n",
    "    \"Bag of Words (min_df=5)\": CountVectorizer(min_df=5),\n",
    "    \"Bag of Words (min_df=10)\": CountVectorizer(min_df=10),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    model = svm.LinearSVC()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names=map_labels.keys())\n",
    "    results[name] = report\n",
    "    print(f\"Classification Report for {name}:\\n\", report)\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b - Answers\n",
    "- overall, the 'negative' category generally performs the best across all three settings. It consistently had the highest precision, the highest recall (in most cases) and pretty high f1 scores. This is likely because negative sentiment is usually a bit more obvious or not nuanced with figure of speech.\n",
    "- Increasing the min_df threshold reduces the number of rare terms used as features, improving precision by focusing on more common, meaningful words. However, this can lower recall, especially for the \"neutral\" category, as some more subtle and infrequent terms that help in classification are discarded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1.4690777883076387 asap\n",
      "0 1.241308538564648 claim\n",
      "0 1.2015175716055708 hrs\n",
      "0 1.1769438379463633 unfriendlyskies\n",
      "0 1.176928569068641 unbelievable\n",
      "0 1.159318888032922 american\n",
      "0 1.1408949690713348 however\n",
      "0 1.0876826508621658 read\n",
      "0 1.0675389091708476 worst\n",
      "0 1.0401153022598528 meanwhile\n",
      "0 1.039570877784889 neverflyvirginforbusiness\n",
      "0 1.023150492074792 business\n",
      "0 0.9554498277015895 broken\n",
      "0 0.9250854205229461 awful\n",
      "0 0.9201533437635246 biz\n",
      "0 0.9201044133564622 why\n",
      "0 0.909130099702888 respond\n",
      "0 0.8867738468101205 delayed\n",
      "0 0.8799617259127095 luck\n",
      "0 0.8576493517584917 dulles\n",
      "0 0.8467075182598232 cold\n",
      "0 0.8420005467312716 doesnt\n",
      "0 0.8231861933004572 didn\n",
      "0 0.8073019535788502 due\n",
      "0 0.7925975734622633 hello\n",
      "0 0.7874944450031423 delays\n",
      "0 0.7869639623317127 told\n",
      "0 0.7851611148026164 hasn\n",
      "0 0.777419932535607 united\n",
      "0 0.7698594722284293 terrible\n",
      "0 0.7694478564936047 problem\n",
      "0 0.7656576475767011 until\n",
      "0 0.7654131242566585 missing\n",
      "0 0.7637506511405227 says\n",
      "0 0.7603348725015435 other\n",
      "0 0.7524575819773469 rebook\n",
      "0 0.749946972057729 late\n",
      "0 0.7481408722967909 worse\n",
      "0 0.7471089155134873 incentive\n",
      "0 0.7414448762440353 delaying\n",
      "0 0.7347577026331209 unhelpful\n",
      "0 0.7274826766359526 suck\n",
      "0 0.721664551417559 no\n",
      "0 0.7139041612799695 phone\n",
      "0 0.7112761615385124 cle\n",
      "0 0.7067952254021851 whole\n",
      "0 0.7015345698743459 paid\n",
      "0 0.6973055764330102 innovation\n",
      "0 0.6959899237469965 5957\n",
      "0 0.6959587530470254 trip\n",
      "0 0.6933513067418043 overflight\n",
      "0 0.6909341936224815 process\n",
      "0 0.6892009834247692 asked\n",
      "0 0.6816857343149627 also\n",
      "0 0.6810604129919312 besides\n",
      "0 0.6791646142190675 pic\n",
      "0 0.6787229277848315 delta\n",
      "0 0.6786663372379625 under\n",
      "0 0.6735534089320102 luggage\n",
      "0 0.6710582700410611 holders\n",
      "0 0.6670411419124788 horrible\n",
      "0 0.660572412094321 lose\n",
      "0 0.659020283648864 bag\n",
      "0 0.6588413965576754 cs\n",
      "0 0.6586899849453944 bos\n",
      "0 0.6586502792975529 unfortunately\n",
      "0 0.6240926298976776 but\n",
      "0 0.620031534989396 self\n",
      "0 0.6200293862135601 policies\n",
      "0 0.6193371295593377 lack\n",
      "0 0.6146579496723026 funny\n",
      "0 0.6131376178065772 expensive\n",
      "0 0.6104502243548187 wasn\n",
      "0 0.6021569674172172 haven\n",
      "0 0.6020233248006821 roc\n",
      "0 0.5995978044826084 delay\n",
      "0 0.5955707172790423 birthday\n",
      "0 0.5953987568819221 ones\n",
      "0 0.5946201186864052 same\n",
      "0 0.5927912666038221 understand\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1.9496200593207547 anytime\n",
      "1 1.7679578903853528 res\n",
      "1 1.5259716121194975 feedback\n",
      "1 1.464136380215503 school\n",
      "1 1.3763013164175968 eco\n",
      "1 1.3718204371579623 weekend\n",
      "1 1.3460037377957583 rikrik__\n",
      "1 1.312917694026916 pick\n",
      "1 1.282451008121048 hiring\n",
      "1 1.2278153150631461 ladies\n",
      "1 1.2116333619185151 believe\n",
      "1 1.2115119120498905 hi\n",
      "1 1.2032050038764153 rate\n",
      "1 1.1413465193470635 757\n",
      "1 1.128666348029495 requires\n",
      "1 1.1218978305706526 following\n",
      "1 1.1065526424879535 round\n",
      "1 1.084971211142131 submitted\n",
      "1 1.0596760453187901 fact\n",
      "1 1.0586299537669435 dtw\n",
      "1 1.0562414543510623 simply\n",
      "1 1.0545123166221502 copy\n",
      "1 1.0513564183004156 apologize\n",
      "1 1.0313645479736682 swiss\n",
      "1 1.0312312226703133 operation\n",
      "1 1.0232415507867036 435\n",
      "1 1.0211845787632094 snowy\n",
      "1 1.0123655725464866 glasses\n",
      "1 1.0075876540338298 tickets\n",
      "1 1.0019657942161642 winter\n",
      "1 1.0007941331399155 tweets\n",
      "1 0.9986770252890288 annricord\n",
      "1 0.9944747176547721 787\n",
      "1 0.974154469716571 place\n",
      "1 0.9632661127965263 info\n",
      "1 0.9537346699363937 deep\n",
      "1 0.9537223644931917 com\n",
      "1 0.9464639714136216 states\n",
      "1 0.9345031780620336 page\n",
      "1 0.9200991668507323 cause\n",
      "1 0.9184624265836026 knew\n",
      "1 0.9124133445055538 failed\n",
      "1 0.9107352697180648 gates\n",
      "1 0.9067258987190951 evening\n",
      "1 0.8964893484168286 fort\n",
      "1 0.8900618649640393 allow\n",
      "1 0.8897354179584739 eastern\n",
      "1 0.8876107950003935 between\n",
      "1 0.8826388913544568 write\n",
      "1 0.8797889900362682 fortunately\n",
      "1 0.8738951276462966 dca\n",
      "1 0.8720575223571172 plate\n",
      "1 0.8686493188815356 prove\n",
      "1 0.8652403964585454 current\n",
      "1 0.8458712206236487 originally\n",
      "1 0.8357443325514352 portland\n",
      "1 0.8331727107100896 ed\n",
      "1 0.8320854908490011 policy\n",
      "1 0.8296662059598141 issued\n",
      "1 0.8226228619764978 fyi\n",
      "1 0.8143474951666003 summer\n",
      "1 0.8089697167467494 speaking\n",
      "1 0.8041897728885199 carousel\n",
      "1 0.7932052291482824 followed\n",
      "1 0.7929611956854736 passport\n",
      "1 0.7910963527610722 exactly\n",
      "1 0.7901651874857445 him\n",
      "1 0.7899735891338715 military\n",
      "1 0.7828419886182322 offer\n",
      "1 0.7812199852966994 dfw\n",
      "1 0.778295444175528 question\n",
      "1 0.7770474111683734 plz\n",
      "1 0.7760350427238577 february\n",
      "1 0.7760008366709991 24th\n",
      "1 0.7759114887469716 mom\n",
      "1 0.7709047665125288 some\n",
      "1 0.7680783226784671 locator\n",
      "1 0.7657242469028422 watching\n",
      "1 0.7626542118156562 bf\n",
      "1 0.7618909495973674 play\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1.6775840227877883 comes\n",
      "2 1.5644374534649748 although\n",
      "2 1.5404277066980763 kudos\n",
      "2 1.5072503102591206 awesome\n",
      "2 1.475552613579246 amazing\n",
      "2 1.446638232700013 love\n",
      "2 1.4457220089248766 favorite\n",
      "2 1.4426333425755624 thnx\n",
      "2 1.4339519841197095 thanks\n",
      "2 1.4317888147721973 despite\n",
      "2 1.3761769667509076 great\n",
      "2 1.3723661612637834 friendly\n",
      "2 1.358464770796239 excellent\n",
      "2 1.351789233963477 thank\n",
      "2 1.35114531223882 glad\n",
      "2 1.3182326495086818 beautiful\n",
      "2 1.3158069413656928 else\n",
      "2 1.2898458107312665 perfect\n",
      "2 1.2366127757959786 lovely\n",
      "2 1.2274531752350868 excited\n",
      "2 1.2151482717404611 ill\n",
      "2 1.2076941956996707 definitely\n",
      "2 1.195339628671943 best\n",
      "2 1.18979603106781 jp\n",
      "2 1.1691040556041004 salt\n",
      "2 1.1646448895278347 loved\n",
      "2 1.1489914334220799 happy\n",
      "2 1.1467076720505707 worries\n",
      "2 1.1429628372657714 wonderful\n",
      "2 1.1427986328512985 sweet\n",
      "2 1.1419588851692386 above\n",
      "2 1.1014891870042456 bird\n",
      "2 1.0959388511491885 imagine\n",
      "2 1.0893527068338484 impressed\n",
      "2 1.0844137641577636 honest\n",
      "2 1.0672580620573202 thx\n",
      "2 1.0481590143599033 fav\n",
      "2 1.0419403390195277 worried\n",
      "2 1.040752154340753 rock\n",
      "2 1.0299248014057178 finest\n",
      "2 1.0244338955992953 usair\n",
      "2 1.011167621667351 whoa\n",
      "2 1.001585814608457 thankyou\n",
      "2 1.0007468693624593 worked\n",
      "2 0.9821677575537079 fine\n",
      "2 0.9797179085866731 appreciated\n",
      "2 0.9644968722930208 stat\n",
      "2 0.9613161503673403 apple\n",
      "2 0.9535634497244962 pleasure\n",
      "2 0.9494295988049724 lets\n",
      "2 0.9438141186970462 loving\n",
      "2 0.9339255870903995 choice\n",
      "2 0.9274828131413214 wow\n",
      "2 0.9257214769884234 confirmed\n",
      "2 0.9133532463452393 course\n",
      "2 0.9085649548170717 enjoy\n",
      "2 0.8953107874816888 happily\n",
      "2 0.8951757888303742 congratulations\n",
      "2 0.8931322651067392 absolutely\n",
      "2 0.8898633213823434 cancun\n",
      "2 0.8824559671858864 flightling\n",
      "2 0.8714237426384476 ahead\n",
      "2 0.8667158159362618 lounge\n",
      "2 0.8581078052294367 pretzels\n",
      "2 0.8404317244760239 thankful\n",
      "2 0.8398610848681133 utah\n",
      "2 0.8381713167241017 thinking\n",
      "2 0.8363327951085502 haul\n",
      "2 0.836229129402821 short\n",
      "2 0.83394234528724 btw\n",
      "2 0.83380662463291 complete\n",
      "2 0.8290399226626534 incredible\n",
      "2 0.8289226652223677 john\n",
      "2 0.8279596463392443 flightlation\n",
      "2 0.8273095532631868 indianapolis\n",
      "2 0.8111536245293754 allows\n",
      "2 0.8029213527098661 friendliest\n",
      "2 0.7993906380469594 heading\n",
      "2 0.7989083753415079 sjc\n",
      "2 0.7979750843017047 route\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "def important_features_per_class(vectorizer, classifier, n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names), reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[1], feature_names), reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.coef_[2], feature_names), reverse=True)[:n]\n",
    "    \n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# Call the function to display important features\n",
    "important_features_per_class(vectorizer, classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
