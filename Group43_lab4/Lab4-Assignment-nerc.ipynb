{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  - a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('C:/Users/lisam/OneDrive/Year 3/Period 4/Text Mining/ba-text-mining-gr43/lab_sessions/lab4/CONLL2003/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {'words':token,'pos':pos}\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('C:/Users/lisam/OneDrive/Year 3/Period 4/Text Mining/ba-text-mining-gr43/lab_sessions/lab4/CONLL2003/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {'words':token,'pos':pos}\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 2, 3: 1, 5: 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "my_list=[1,2,1,3,2,5]\n",
    "Counter(my_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:\n",
      "Number of instances: 203621\n",
      "\n",
      "Frequency distribution of NERC labels:\n",
      "B-ORG: 6321\n",
      "O: 169578\n",
      "B-MISC: 3438\n",
      "B-PER: 6600\n",
      "I-PER: 4528\n",
      "B-LOC: 7140\n",
      "I-ORG: 3704\n",
      "I-MISC: 1155\n",
      "I-LOC: 1157\n",
      "\n",
      "Dstribution of NERC labels (in percentage):\n",
      "B-ORG: 3.10%\n",
      "O: 83.28%\n",
      "B-MISC: 1.69%\n",
      "B-PER: 3.24%\n",
      "I-PER: 2.22%\n",
      "B-LOC: 3.51%\n",
      "I-ORG: 1.82%\n",
      "I-MISC: 0.57%\n",
      "I-LOC: 0.57%\n",
      "\n",
      "Test:\n",
      "Number of instances: 46435\n",
      "\n",
      "Frequency distribution of NERC labels:\n",
      "O: 38323\n",
      "B-LOC: 1668\n",
      "B-PER: 1617\n",
      "I-PER: 1156\n",
      "I-LOC: 257\n",
      "B-MISC: 702\n",
      "I-MISC: 216\n",
      "B-ORG: 1661\n",
      "I-ORG: 835\n",
      "\n",
      "Dstribution of NERC labels (in percentage):\n",
      "O: 82.53%\n",
      "B-LOC: 3.59%\n",
      "B-PER: 3.48%\n",
      "I-PER: 2.49%\n",
      "I-LOC: 0.55%\n",
      "B-MISC: 1.51%\n",
      "I-MISC: 0.47%\n",
      "B-ORG: 3.58%\n",
      "I-ORG: 1.80%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "path = 'C:/Users/lisam/OneDrive/Year 3/Period 4/Text Mining/ba-text-mining-gr43/lab_sessions/lab4/CONLL2003/CONLL2003'\n",
    "\n",
    "train = ConllCorpusReader(path, 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "test = ConllCorpusReader(path, 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "def count_labels(data):\n",
    "    \n",
    "    labels = []\n",
    "    for token, pos, ne_label in data.iob_words():\n",
    "        labels.append(ne_label)\n",
    "    return labels, Counter(labels)\n",
    "\n",
    "def calculate_percentages(label_counts, total):\n",
    "    print(\"\\nDstribution of NERC labels (in percentage):\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{label}: {percentage:.2f}%\")\n",
    "\n",
    "def get_statistics(data, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    labels, label_counts = count_labels(data)\n",
    "    num_instances = len(labels)\n",
    "\n",
    "    print(f\"Number of instances: {num_instances}\")\n",
    "    \n",
    "    print(\"\\nFrequency distribution of NERC labels:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "    calculate_percentages(label_counts, num_instances)\n",
    "\n",
    "get_statistics(train, \"Train\")\n",
    "get_statistics(test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "- There are 203621 instances of training data, and 46435 of testing, as it can be seen from the output of the cell below.\n",
    "- The frequency distibution can also be seen in the output of the cell below, which for training data is:\n",
    "B-ORG: 6321\n",
    "\n",
    "O: 169578\n",
    "\n",
    "B-MISC: 3438\n",
    "\n",
    "B-PER: 6600\n",
    "\n",
    "I-PER: 4528\n",
    "\n",
    "B-LOC: 7140\n",
    "\n",
    "I-ORG: 3704\n",
    "\n",
    "I-MISC: 1155\n",
    "\n",
    "I-LOC: 1157\n",
    "\n",
    "and for test data is:\n",
    "\n",
    "O: 38323\n",
    "\n",
    "B-LOC: 1668\n",
    "\n",
    "B-PER: 1617\n",
    "\n",
    "I-PER: 1156\n",
    "\n",
    "I-LOC: 257\n",
    "\n",
    "B-MISC: 702\n",
    "\n",
    "I-MISC: 216\n",
    "\n",
    "B-ORG: 1661\n",
    "\n",
    "I-ORG: 835\n",
    "\n",
    "- The part with percentage count in the output of the cell below clearly indicates the imbalance of training and test data when looking at them separately. For both test and train data, the \"O\" label, which is used to label the words that are not a part of any category of named entity, dominates with 83.28% and 82.53%, respectively. This can lead to biases in the model and incorrect labeling. And looking at percentage of labels that start with I or B and comparing them, we can see that in general, NERC labels that start with B ahave higher percentages, which means that the most named entities are shorter rather than longer. If comparing test and training data with each other, they do have similar percentages for the same labels, not having differences of more than ~1%, some of them having a difference only of 0.02% (for example I-LOC, with 0.57% and 0.55%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 203621\n",
      "Test examples: 46435\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/lisam/OneDrive/Year 3/Period 4/Text Mining/ba-text-mining-gr43/lab_sessions/lab4/CONLL2003/CONLL2003'\n",
    "reader = ConllCorpusReader(path, ['train.txt', 'test.txt', 'valid.txt'], ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "training_features = []\n",
    "training_labels = []\n",
    "\n",
    "for token, pos, ne_label in reader.iob_words('train.txt'):\n",
    "    training_features.append({'words': token, 'pos': pos})\n",
    "    training_labels.append(ne_label)\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for token, pos, ne_label in reader.iob_words('test.txt'):\n",
    "    test_features.append({'words': token, 'pos': pos})\n",
    "    test_labels.append(ne_label)\n",
    "\n",
    "print(f\"Training examples: {len(training_features)}\")\n",
    "print(f\"Test examples: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train array shape: (203621, 27361)\n",
      "Test array shape: (46435, 27361)\n"
     ]
    }
   ],
   "source": [
    "all_features = training_features + test_features\n",
    "\n",
    "vec = DictVectorizer()\n",
    "the_array = vec.fit_transform(all_features)\n",
    "\n",
    "num_train = len(training_features)\n",
    "train_array = the_array[:num_train]\n",
    "test_array = the_array[num_train:]\n",
    "\n",
    "print(\"Train array shape:\", train_array.shape)\n",
    "print(\"Test array shape:\", test_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lisam\\anaconda3\\envs\\TextMining\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.77      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.71       702\n",
      "       B-ORG       0.79      0.52      0.62      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.59      0.59      0.59       216\n",
      "       I-ORG       0.66      0.48      0.55       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.99      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.71      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_features = training_features + test_features\n",
    "all_labels = training_labels + test_labels\n",
    "\n",
    "vec = DictVectorizer()\n",
    "X_all = vec.fit_transform(all_features)\n",
    "\n",
    "X_train = X_all[:len(training_features)]\n",
    "X_test = X_all[len(training_features):]\n",
    "\n",
    "y_train = training_labels\n",
    "y_test = test_labels\n",
    "\n",
    "lin_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "* <u>Which NERC labels does the classifier perform well on? Why do you think this is the case?</u>\n",
    "    \n",
    "    The classifier performs well on O labels, probably getting a high F1-score because there is a lot of them available and they're easy to tell from named entities. It also performs well on B-LOC and B-MISC, likely because locations and miscellaneous entities follow recognizable patterns.\n",
    "\n",
    "* <u>Which NERC labels does the classifier perform poorly on? Why do you think this is the case?</u>\n",
    "    \n",
    "    However, it struggles with inside entity labels, especially I-PER, which has high recall but low precision, indicating over-prediction. B-PER also has low recall, meaning many person names are missed. This is likely due to the variability of personal names and insufficient training data for certain entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Training SVM...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.77      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.71       702\n",
      "       B-ORG       0.79      0.52      0.62      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.59      0.59      0.59       216\n",
      "       I-ORG       0.66      0.48      0.55       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.99      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.71      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lisam\\anaconda3\\envs\\TextMining\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "def get_embedding(word):\n",
    "    return glove_model[word] if word in glove_model else np.zeros(50)\n",
    "\n",
    "X_train = X_all[:len(training_features)]\n",
    "X_test = X_all[len(training_features):]\n",
    "\n",
    "y_encoder = LabelEncoder()\n",
    "y_train = y_encoder.fit_transform(training_labels)\n",
    "y_test = y_encoder.transform(test_labels)\n",
    "\n",
    "print(\"Training SVM...\")\n",
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "* <u>compare the results with the classifier you built in 2d.</u>\n",
    "\n",
    "    The results are identical and have no differences between them which could mean that these traditional features already captured enough information for NER, so adding word embedding didn't help. GloVe embedding might also not be well-suited for entity recognition because word embeddings focus on general semantic meaning but NER requires context information. Maybe the SVM model might not effectively leverage dense word embeddings compared to deep learning model like BiLTSMs or Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'C:/Users/lisam/OneDrive/Year 3/Period 4/Text Mining/ba-text-mining-gr43/lab_sessions/lab4/ner_dataset.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, encoding='latin1',       on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 1000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:5000]\n",
    "df_test = kaggle_dataset[5000:6000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_and_labels(data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        word = row['Word']\n",
    "        pos = row['POS']\n",
    "        ne_label = row['Tag']\n",
    "        \n",
    "        \n",
    "        feature_dict = {'word': word, 'pos': pos}\n",
    "        features.append(feature_dict)\n",
    "        labels.append(ne_label)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "train_features, train_labels = create_features_and_labels(df_train)\n",
    "test_features, test_labels = create_features_and_labels(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "all_features = train_features + test_features\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "all_features_vectorized = vectorizer.fit_transform(all_features)\n",
    "train_features_vectorized = all_features_vectorized[:len(train_features)]\n",
    "test_features_vectorized = all_features_vectorized[len(train_features):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;linear&#x27;, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', verbose=True)\n",
    "svm_classifier.fit(train_features_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         7\n",
      "       B-eve       1.00      0.25      0.40         4\n",
      "       B-geo       0.19      0.86      0.31        22\n",
      "       B-gpe       0.89      0.46      0.60        35\n",
      "       B-nat       1.00      0.33      0.50         3\n",
      "       B-org       0.82      0.39      0.53        23\n",
      "       B-per       1.00      0.36      0.53        14\n",
      "       B-tim       0.92      0.86      0.89        14\n",
      "       I-art       0.00      0.00      0.00         5\n",
      "       I-eve       0.00      0.00      0.00         3\n",
      "       I-geo       0.00      0.00      0.00         1\n",
      "       I-gpe       1.00      0.80      0.89         5\n",
      "       I-nat       0.00      0.00      0.00         2\n",
      "       I-org       0.89      0.57      0.70        14\n",
      "       I-per       0.67      0.11      0.18        19\n",
      "           O       0.99      1.00      1.00       829\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.59      0.37      0.41      1000\n",
      "weighted avg       0.94      0.91      0.91      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lisam\\anaconda3\\envs\\TextMining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lisam\\anaconda3\\envs\\TextMining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lisam\\anaconda3\\envs\\TextMining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_predictions = svm_classifier.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answser\n",
    "* <u>Evaluate the performance of the SVM on the test data</u>\n",
    "\n",
    "    The performance of the SVM classifier can be evaluated using precision, recall, and F1-score for each NERC label. \n",
    "    Considering the classification report, the classifier performs well on labels like B-tim and O, as they have high precision and recall. \n",
    "    This is likely because location names are well-defined and frequently seen in training data.\n",
    "\n",
    "    However, the classifier struggles with labels like B-org  and B-per, showing lower recall. \n",
    "    This could be due to the variability in organization and person names, which may not follow strict patterns. \n",
    "    The I-per label especially has poor precision but high recall, indicating that the model often makes mistakes with non-person continuation tokens as part of a person entity. \n",
    "    These errors may be a reason for difficulty in distinguishing multi-token named entities, leading to incomplete entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
